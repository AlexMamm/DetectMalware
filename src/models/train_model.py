import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import numpy as np
import os
import pickle
from sklearn.preprocessing import LabelEncoder
import datetime


class Data(Dataset):
    def __init__(self, x_in, y_in):
        x_in = torch.from_numpy(x_in)
        self.x = torch.unsqueeze(x_in, dim=1)
        self.y = torch.from_numpy(np.array(y_in))

        self.len = self.x.shape[0]

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return self.len


class CNN(torch.nn.Module):
    def __init__(self, classes):
        super(CNN, self).__init__()

        self.convolutional_layers = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=3, stride=2),
            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=3, stride=2),
            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d(kernel_size=3, stride=2)
        )
        self.dense_layers = torch.nn.Sequential(
            torch.nn.Linear(128 * 3 * 3, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.5),
            torch.nn.Linear(256, 128),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.5),
            torch.nn.Linear(128, classes)
        )

    def forward(self, X):
        out = self.convolutional_layers(X)
        out = out.view(out.size(0), -1)
        out = self.dense_layers(out)
        return out


def train_test_loader(X, y, batch_size):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.80, stratify=y, random_state=42)

    count_classes = np.unique(y).shape[0]
    train_data, test_data = Data(X_train, y_train), Data(X_test, y_test)
    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)

    return train_loader, test_loader, count_classes


def train(root, type_of_classifier, train_loader, test_loader, count_classes, epochs):
    torch.cuda.is_available = lambda: False
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = CNN(classes=count_classes)
    loss = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())

    train_losses = np.zeros(epochs)
    test_losses = np.zeros(epochs)

    for it in range(epochs):
        model.train()
        t0 = datetime.datetime.now()
        train_loss = []
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            losses = loss(outputs, targets)
            losses.backward()
            optimizer.step()
            train_loss.append(losses.item())
        train_loss = np.mean(train_loss)

        model.eval()
        test_loss = []
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            losses = loss(outputs, targets)
            test_loss.append(losses.item())
        test_loss = np.mean(test_loss)

        train_losses[it] = train_loss
        test_losses[it] = test_loss

        dt = datetime.datetime.now() - t0
        print(f'Epoch {it + 1}/{epochs}  Train Loss: {train_loss:.4f}  Test Loss: {test_loss:.4f}  Duration: {dt}')

    torch.save(model, os.path.join(root, 'models', type_of_classifier + '_classifier.pickle'))
    print('\n[+] Training done!')


def learn_model(root, X, y, type_of_classifier, batch_size, epochs):
    X_load, y_load = [], []
    if type_of_classifier == 'binary':
        for x_elem, y_elem in zip(X, y):
            if y_elem == 'benign':
                X_load.append(x_elem)
                y_load.append('benign')
            elif y_elem != 'nan':
                X_load.append(x_elem)
                y_load.append('malicious')
    elif type_of_classifier == 'multi':
        for x_elem, y_elem in zip(X, y):
            if y_elem != 'benign' and y_elem != 'nan':
                X_load.append(x_elem)
                y_load.append(y_elem)
    else:
        print('[!] Invalid classifier type')
        return 1

    print(f'[...] {type_of_classifier} classifier training!')
    encoder = LabelEncoder()
    X_load = np.array(X_load)
    y_load = encoder.fit_transform(y_load)
    print(f'[...] Shape dataset (X,y): ({X_load.shape}, {y_load.shape})')
    pickle.dump(encoder, open(os.path.join(root, 'models', type_of_classifier + '_encoder.pickle'), 'wb'))

    train_loader, test_loader, count_classes = train_test_loader(X_load, y_load, batch_size)
    train(root, type_of_classifier, train_loader, test_loader, count_classes, epochs)


def learn(root):
    X = pickle.load(open(os.path.join(root, 'models', 'X_data.pickle'), 'rb'))
    y = pickle.load(open(os.path.join(root, 'models', 'y_data.pickle'), 'rb'))

    learn_model(root, X, y, 'binary', 128, 50)
    learn_model(root, X, y, 'multi', 128, 50)


if __name__ == '__main__':
    ROOT_DIR = '/'.join(os.path.dirname(os.path.abspath(__file__)).split('/')[:-2])
    learn(ROOT_DIR)
